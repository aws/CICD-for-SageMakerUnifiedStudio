{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retail Sales Forecasting with SageMaker XGBoost\n",
    "\n",
    "This notebook demonstrates end-to-end retail sales forecasting using Amazon SageMaker's managed XGBoost container. We'll cover:\n",
    "\n",
    "1. **Data preprocessing** - Load and prepare retail sales data\n",
    "2. **Feature engineering** - Create time-series features for forecasting\n",
    "3. **Model training** - Train XGBoost model using SageMaker\n",
    "4. **Hyperparameter tuning** - Optimize model performance automatically\n",
    "5. **Batch inference** - Generate predictions on test data\n",
    "6. **Resource cleanup** - Clean up resources\n",
    "\n",
    "## Dataset Attribution\n",
    "Â© Chen, D. (2012). Online Retail II [Dataset]. UCI Machine Learning Repository. Available at: https://archive.ics.uci.edu/dataset/502/online+retail+ii. Licensed under Creative Commons Attribution 4.0 International (CC BY 4.0) license, which can be found here: https://creativecommons.org/licenses/by/4.0/legalcode#s6a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn xgboost boto3 joblib\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "# Initialize boto3 session\n",
    "session = boto3.Session()\n",
    "s3_client = boto3.client('s3')\n",
    "region = session.region_name or 'us-west-2'\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "# S3 bucket for data storage\n",
    "bucket = f'sagemaker-{region}-{account_id}'\n",
    "prefix = 'retail-sales-forecasting'\n",
    "\n",
    "# Create unique session ID for resource tracking\n",
    "session_id = f\"{int(time.time())}\"\n",
    "print(f\"Session ID: {session_id} (for resource cleanup)\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "current_region = boto3.Session().region_name or \"us-west-2\"\n",
    "data_url = f\"s3://sagemaker-example-files-prod-{current_region}/datasets/tabular/online_retail/online_retail_II_20k.csv\"\n",
    "df = pd.read_csv(data_url)\n",
    "df = df.dropna(subset=[\"Customer ID\"])\n",
    "df = df[(df[\"Quantity\"] > 0) & (df[\"Price\"] > 0)]\n",
    "df[\"InvoiceDate\"] = pd.to_datetime(df[\"InvoiceDate\"])\n",
    "df[\"Revenue\"] = df[\"Quantity\"] * df[\"Price\"]\n",
    "\n",
    "# Aggregate daily sales\n",
    "daily_sales = df.groupby(df['InvoiceDate'].dt.date).agg({\n",
    "    'Revenue': 'sum', 'Quantity': 'sum', 'Invoice': 'nunique', 'Customer ID': 'nunique'\n",
    "}).reset_index()\n",
    "daily_sales.columns = ['Date', 'Revenue', 'Quantity', 'Orders', 'Customers']\n",
    "daily_sales['Date'] = pd.to_datetime(daily_sales['Date'])\n",
    "daily_sales = daily_sales.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "print(f'Summary of Revenue')\n",
    "print(daily_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Quarter'] = df['Date'].dt.quarter\n",
    "    df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(int)\n",
    "    \n",
    "    # Reduced lag features to preserve data\n",
    "    for lag in [1, 2]:\n",
    "        df[f'Revenue_lag_{lag}'] = df['Revenue'].shift(lag)\n",
    "    \n",
    "    # Smaller rolling window\n",
    "    df['Revenue_ma_3'] = df['Revenue'].rolling(window=3).mean()\n",
    "    \n",
    "    return df\n",
    "\n",
    "daily_sales_features = create_features(daily_sales)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "daily_sales_features = daily_sales_features.dropna().reset_index(drop=True)\n",
    "\n",
    "if len(daily_sales_features) < 5:\n",
    "    raise ValueError(f\"Insufficient data after feature engineering: {len(daily_sales_features)} rows\")\n",
    "\n",
    "print(f\"Final dataset has {len(daily_sales_features)} rows for modeling\")\n",
    "print(daily_sales_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "From the Final Datasets that has 6 rows, first 4 are used for training and 2 will be used as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "feature_cols = [col for col in daily_sales_features.columns if col not in ['Date', 'Revenue']]\n",
    "print(f\"Feature columns: {feature_cols}\")\n",
    "\n",
    "split_idx = int(len(daily_sales_features) * 0.8)\n",
    "train_data = daily_sales_features[:split_idx]\n",
    "test_data = daily_sales_features[split_idx:]\n",
    "\n",
    "print(f\"Train data shape: {train_data.shape}, Test data shape: {test_data.shape}\")\n",
    "\n",
    "# Prepare features and target\n",
    "X_train = train_data[feature_cols]\n",
    "y_train = train_data['Revenue']\n",
    "X_test = test_data[feature_cols]\n",
    "y_test = test_data['Revenue']\n",
    "\n",
    "print(f\"Training data: {len(train_data)} days, Test data: {len(test_data)} days\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "os.makedirs(\"notebook_outputs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training with Local XGBoost\n",
    "\n",
    "Training XGBoost model locally for faster iteration and simpler deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train initial XGBoost model locally\n",
    "print(\"Training initial XGBoost model...\")\n",
    "\n",
    "# Create DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Set parameters\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "# Train model\n",
    "evals = [(dtrain, 'train'), (dtest, 'test')]\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=100,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=20\n",
    ")\n",
    "\n",
    "print(f\"\\nModel training completed!\")\n",
    "print(f\"Best iteration: {model.best_iteration}\")\n",
    "print(f\"Best score: {model.best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning using GridSearchCV\n",
    "print(\"Starting hyperparameter tuning...\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'eta': [0.01, 0.1, 0.3],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Create XGBoost regressor\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Perform grid search (using a subset of parameters for speed)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid={\n",
    "        'max_depth': [3, 6, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.3]\n",
    "    },\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score (negative MSE): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Use best model\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"\\nBest model selected from grid search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batch Inference\n",
    "\n",
    "Using the best model from tuning, we'll generate predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions using the best model\n",
    "print(\"Generating predictions on test data...\")\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "print(f\"Generated {len(predictions)} predictions\")\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictions\n",
    "actual_values = y_test.values\n",
    "predicted_values = predictions\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(actual_values, predicted_values)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(actual_values, predicted_values)\n",
    "\n",
    "print(f\"Model Performance Metrics:\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "\n",
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': actual_values,\n",
    "    'Predicted': predicted_values,\n",
    "    'Error': actual_values - predicted_values\n",
    "})\n",
    "\n",
    "print(\"\\nPrediction Results:\")\n",
    "print(results_df)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df.index, results_df['Actual'], 'o-', label='Actual', linewidth=2)\n",
    "plt.plot(results_df.index, results_df['Predicted'], 's-', label='Predicted', linewidth=2)\n",
    "plt.xlabel('Test Sample')\n",
    "plt.ylabel('Revenue')\n",
    "plt.title('Actual vs Predicted Revenue')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Save predictions\n",
    "results_df.to_csv('notebook_outputs/predictions.csv', index=False)\n",
    "print(\"\\nPredictions saved to notebook_outputs/predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "### ðŸš€ **How to Improve Prediction Accuracy**\n",
    "\n",
    "1. **Larger Training Dataset**: Use full historical data (months/years vs. sample data)\n",
    "2. **Advanced Features**: Add external factors (holidays, promotions, weather)\n",
    "3. **Algorithm Selection**: Try DeepAR, Prophet, or ensemble methods\n",
    "4. **Data Quality**: Handle missing values and remove outliers\n",
    "\n",
    "### âš¡ **How to Improve Performance & Speed**\n",
    "\n",
    "1. **Infrastructure**: Use larger instances or distributed training\n",
    "2. **Optimization**: Enable early stopping and cache preprocessed data\n",
    "3. **Real-time**: Deploy to SageMaker endpoints with auto-scaling\n",
    "\n",
    "### ðŸŽ¯ **Next Steps for Production**\n",
    "\n",
    "1. **MLOps**: Set up automated retraining with SageMaker Pipelines\n",
    "2. **Monitoring**: Implement data drift detection\n",
    "3. **Integration**: Connect with business systems (ERP, inventory)\n",
    "\n",
    "### ðŸ’¡ **Business Impact**\n",
    "\n",
    "Accurate forecasting enables inventory optimization, better resource planning, improved financial planning, and higher customer satisfaction through product availability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resource Clean up\n",
    "\n",
    "Clean up local resources created by this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up local resources\n",
    "print('Cleaning up local resources created by this notebook...')\n",
    "\n",
    "# Clean up local files created by this notebook\n",
    "files_to_delete = ['predictions.csv', 'churn.txt']\n",
    "deleted_count = 0\n",
    "\n",
    "for file in files_to_delete:\n",
    "    file_path = os.path.join('notebook_outputs', file)\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f'Deleted {file_path}')\n",
    "        deleted_count += 1\n",
    "\n",
    "print(f'\\nDeleted {deleted_count} local files')\n",
    "print('Cleanup completed - only local resources created by this notebook were removed!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
